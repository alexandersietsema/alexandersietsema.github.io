---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---
<i>Last updated: Feb 2026</i>

Machine learning is a rapidly evolving field. State-of-the-art methods often develop much more quickly than either their properties can be understood theoretically or their effects in practice adequately studied. My research focuses on addressing these concerns: first, I work on better understanding the theoretical foundations of machine learning, especially in understanding where and how existing algorithms fail to provide good results, and finding ways to mitigate those limitations. I also work on developing new algorithms to better address the constraints involved in specific applications. I explore these problems at the intersection of theory and practice, using tools from numerical linear algebra, convex optimization, and statistical learning theory. Below are a couple projects that I've been working on recently.

<div style="display:inline-block">
<h2> Harmful Overfitting in Sobolev Spaces </h2>
<img align="left" style="width:35%; margin: 0px 20px 0px 0px;" src="../images/sketch_diagram_2.png">
<p> Conventional machine learning wisdom suggests that overfitting to training data tends to provide poor generalization. However, in many applications, we see that algorithms can both perfectly fit the training data and generalize well to unseen data. This phenomenon is known as <i>benign overfitting</i>. In this work, we consider minimum-norm optimizers of a noisy dataset in Sobolev spaces. Under reasonable assumptions, we prove that even approximately norm-minimizing functions which perfectly fit the training data <i>cannot</i> benignly overfit, that is, they must exhibit some constant, nonzero generalization error independent of the number of data points used. This a significant generalization of previous work, which either works in more restrictive Sobolev spaces or using more restrictive classes of functions (e.g., ReLU networks). This work was submitted in 2026 and is awaiting revision. The preprint is available <a href="https://arxiv.org/abs/2602.00825">here</a>.
</p></div>

<div style="display:inline-block">
<br><br>
<h2> Geometric Deep Learning </h2>
<img align="right" style="width:35%; margin: 0px 0px 0px 20px;" src="../images/manifold_graph_diagram.png">
<p> Graph deep learning has become a critical tool for understanding large, unstructured datasets. Recent work has introduced diffusion wavelet-based methods for graph learning with scalar-valued signals. In many graph problems, however, nodes have vector-valued signals (such as locations in 3D space), and predictions should be invariant to rotations of those signals. For example, predicting wind vectors at a given location should be independent of the orientation chosen for the Earth. In this paper, we develop vector diffusion wavelet graph neural networks (VDW-GNNs) to extend diffusion wavelets to vector-valued signals. We prove that the method is rotationally invariant, and demonstrate improved performance over existing methods on real-world datasets. This work was submitted in 2026 and is awaiting revision.
</p></div>

<div style="display:inline-block">
<h2> Kaczmarz Methods</h2>
<img align="left" style="width:35%; margin: 0px 20px 0px 0px;" src="../images/kaczmarz.png">
<p> In the summer of 2021 I was fortunate enough to participate in the UCLA Computational and Applied Mathematics REU in which I studied Kaczmarz methods with Dr. Jamie Haddock. Throughout my Ph.D. work, I have continued working on Kaczmarz methods for a variety of problem types and and applications, but especially for use in rank aggregation. We recently published a paper (found <a href="https://link.springer.com/article/10.1007/s10543-024-01024-x">here</a>) on applying Kaczmarz methods for pairwise comparison rank aggregation, with applications in recommender systems, information retrieval, sports, and sociology.
</p></div>
